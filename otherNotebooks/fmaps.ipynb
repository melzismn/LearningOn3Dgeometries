{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fmaps.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/melzismn/Digital-Design-2020-2021/blob/master/fmaps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2yRvI6BmMg7"
      },
      "source": [
        "# ONLY FOR COLAB\n",
        "# Not required in Binder\n",
        "\n",
        "!wget -c https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "!bash ./Miniconda3-4.5.4-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!conda install -q -y --prefix /usr/local python=3.6 ujson\n",
        "\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.6/site-packages')\n",
        "\n",
        "import ujson\n",
        "print(ujson.dumps({1:2}))\n",
        "\n",
        "!conda install -c conda-forge igl\n",
        "!conda install -c conda-forge meshplot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpI4m_FcmS2V"
      },
      "source": [
        "!pip install --upgrade --force-reinstall Pillow\n",
        "\n",
        "import igl\n",
        "import scipy\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "from meshplot import plot, subplot, interact\n",
        "import meshplot\n",
        "from scipy.sparse.linalg import eigs,eigsh\n",
        "from scipy.sparse import csr_matrix\n",
        "import os \n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rBPjw-0mXTL"
      },
      "source": [
        "def plot_pair(v1, v2, f1, f2, c1, c2, color_ops = {}):\n",
        "    # Compute a scale factor\n",
        "    M1 = igl.massmatrix(v1, f1, igl.MASSMATRIX_TYPE_VORONOI)\n",
        "    M2 = igl.massmatrix(v2, f2, igl.MASSMATRIX_TYPE_VORONOI)\n",
        "    scale_factor = np.sqrt(np.sum(M2)/np.sum(M1))\n",
        "\n",
        "    # Align the shapes\n",
        "    v2 = v2 - np.mean(v2,axis=0)\n",
        "    v1_align = v1 * scale_factor + np.mean(v1,axis=0) + [0.7,-0.7,0.0]\n",
        "\n",
        "    # Merge the models\n",
        "    v_all = np.vstack((v1_align, v2))\n",
        "    f_all = np.vstack((f1, f2 + np.max(f1)+1))\n",
        "    c_all = np.vstack((c1, c2))\n",
        "    \n",
        "    plot(v_all, f_all, c_all, shading = color_ops)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGY1lpZGai3v"
      },
      "source": [
        "# Load Shapes\n",
        "v_src, f_src = igl.read_triangle_mesh(os.path.join('.', \"data\", \"tr_reg_089.off\"))\n",
        "\n",
        "L_src = -igl.cotmatrix(v_src, f_src)\n",
        "M_src = igl.massmatrix(v_src, f_src, igl.MASSMATRIX_TYPE_VORONOI)\n",
        "\n",
        "try:\n",
        "    evals_src, evecs_src = eigsh(L_src, 200, M_src, sigma=0.0, which='LM', maxiter=1e9, tol=1.e-15)\n",
        "except:\n",
        "    evals_src, evecs_src = eigsh(L_src- 1e-8* scipy.sparse.identity(v_src.shape[0]), 200,\n",
        "                         M_src, sigma=0.0, which='LM', maxiter=1e9, tol=1.e-15)\n",
        "\n",
        "evals_src = evals_src.astype(np.float32)\n",
        "evals_src = evals_src.astype(np.float32)\n",
        "\n",
        "v_tar, f_tar = igl.read_triangle_mesh(os.path.join('.', \"data\", \"tr_reg_090.off\"))\n",
        "\n",
        "L_tar = -igl.cotmatrix(v_tar, f_tar)\n",
        "M_tar = igl.massmatrix(v_tar, f_tar, igl.MASSMATRIX_TYPE_VORONOI)\n",
        "\n",
        "try:\n",
        "    evals_tar, evecs_tar = eigsh(L_tar, 200, M_tar, sigma=0.0, which='LM', maxiter=1e9, tol=1.e-15)\n",
        "except:\n",
        "    evals_tar, evecs_tar = eigsh(L_tar- 1e-8* scipy.sparse.identity(v_tar.shape[0]), 200,\n",
        "                         M_src, sigma=0.0, which='LM', maxiter=1e9, tol=1.e-15)\n",
        "    \n",
        "evals_tar = evals_tar.astype(np.float32)\n",
        "evals_tar = evals_tar.astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXJB3W56qLKr"
      },
      "source": [
        "# WKS\n",
        "\n",
        "We will use the Wave Kernel Signature (WKS) descriptor to do the matching. Recall the formula:\n",
        "\n",
        "$K_E(x,x) = \\sum\\limits_{l=1}^{\\infty}e^{- \\frac{(log(E) - log(\\lambda_l))^2}{2\\sigma^2}} \\phi_l(x)^2 $\n",
        "\n",
        "Where:\n",
        "- $sigma = 7 \\delta$\n",
        "- $delta =  (e_{max} - e{min})/ M$\n",
        "- $e_{max} = log(E_N) - 2\\sigma$\n",
        "- $e_{min} = log(E_1) + 2\\sigma$\n",
        "- $E_N$ is the max eigenvalue in absolute value\n",
        "- $E_1$ is the min non-zero eigenvalue in absolute value\n",
        "- $M$ is the number of WKS scales\n",
        "\n",
        "The tasks are:\n",
        "- Read the meshes, compute the LBO eigenvectors\n",
        "- Define the WKS computation\n",
        "- Visualize the WKS scales on meshes\n",
        "- Perform the matching using WKS (Nearest-Neighbor in the descriptor space)\n",
        "- Visualize the matching (and compute the error)\n",
        "\n",
        "Are the descriptors coherent among the shapes, for different descriptor scales? Is the matching good? We can change the number of descriptors: does it impact the matching?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xa35jY3hpwW5"
      },
      "source": [
        "def WKS(vertices, faces, evals, evecs, wks_size, variance):\n",
        "    # Number of vertices\n",
        "    n = vertices.shape[0]\n",
        "    WKS = np.zeros((n,wks_size))\n",
        "\n",
        "    # Just for numerical stability\n",
        "    evals[evals<1e-6] = 1e-6\n",
        "\n",
        "    # log(E)\n",
        "    log_E = np.log(evals).T\n",
        "\n",
        "    # Define the energies step\n",
        "    e = np.linspace(log_E[1], np.max(log_E)/1.02, wks_size)\n",
        "\n",
        "    # Compute the sigma\n",
        "    sigma = (e[1]-e[0]) * variance\n",
        "    C = np.zeros((wks_size,1))\n",
        "\n",
        "    for i in np.arange(0,wks_size):\n",
        "        # Computing WKS\n",
        "        WKS[:,i] = np.sum(\n",
        "            (evecs)**2 * np.tile( np.exp((-(e[i] - log_E)**2) / (2*sigma**2)),(n,1)), axis=1)\n",
        "        \n",
        "        # Noramlization\n",
        "        C[i] = np.sum(np.exp((-(e[i]-log_E)**2)/(2*sigma**2)))\n",
        "        \n",
        "    WKS = np.divide(WKS,np.tile(C,(1,n)).T)\n",
        "    return WKS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBG2ZKlHp1Jv"
      },
      "source": [
        "# Computing the descriptors for the two shapes\n",
        "d_src = WKS(v_src, f_src, evals_src, evecs_src, 100, 7)\n",
        "d_tar = WKS(v_tar, f_tar, evals_tar, evecs_tar, 100, 7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sfu49Cf01Xec"
      },
      "source": [
        "# Visualizing descriptors\n",
        "i = 2\n",
        "plot_pair(v_src, v_tar, f_src, f_tar, d_src[:,i:i+1], d_tar[:,i:i+1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWHhseWx1ZDm"
      },
      "source": [
        "# Nearest Neighbor\n",
        "treesearch = sp.spatial.cKDTree(d_tar)\n",
        "p2p = treesearch.query(d_src, k=1)[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyRvBccW1aNc"
      },
      "source": [
        "# To see the quality of the matching we plot a function on one shape and we transfer it to the other\n",
        "funz_ = (v_tar - np.min(v_tar,0))/np.tile((np.max(v_tar,0)-np.min(v_tar,0)),(np.size(v_tar,0),1));\n",
        "colors = np.cos(funz_);\n",
        "funz_tar = (colors-np.min(colors))/(np.max(colors) - np.min(colors));\n",
        "funz_src = funz_tar[p2p]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gN3sxDPK1euO"
      },
      "source": [
        "# Plot and (euclidean) error evaluation\n",
        "funz_src = funz_tar[p2p]\n",
        "plot_pair(v_src, v_tar, f_src, f_tar, funz_src,funz_tar)\n",
        "err = np.sum(np.square(v_tar - v_tar[p2p,:]))\n",
        "print(err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkmqC8jZ59S-"
      },
      "source": [
        "# Functional Maps\n",
        "\n",
        "Here we will use the Functional Maps framework. Given some descriptors $d$ on the first shape and $f$ on the second, we will compute our map $C$ such that: \\\n",
        "$ d = \\phi_{src}^{T} A_{src} d $ \\\n",
        "$ f = \\phi_{tar}^{T} A_{tar} f $ \\\n",
        "$ C D = F $ \\\n",
        "$ C =  F D^{-1} $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTyopAGD583y"
      },
      "source": [
        "# WKS\n",
        "n_evals = 5\n",
        "n_desc = 3\n",
        "D = (np.matmul(evecs_src[:,0:n_evals].T, np.matmul(M_src.todense() , d_src[:,0:n_desc])))\n",
        "F = (np.matmul(evecs_tar[:,0:n_evals].T, np.matmul(M_tar.todense() , d_tar[:,0:n_desc])))\n",
        "C =  np.matmul(F,np.linalg.pinv(D))\n",
        "\n",
        "plt.imshow(C)\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObnDcw1x-Ytg"
      },
      "source": [
        "# Compute p2p correspondence\n",
        "treesearch = sp.spatial.cKDTree(np.matmul(evecs_tar[:,0:n_evals],C))\n",
        "p2p = treesearch.query(evecs_src[:,0:n_evals], k=1)[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgk5NvZOAEKa"
      },
      "source": [
        "funz_src = funz_tar[p2p]\n",
        "plot_pair(v_src, v_tar, f_src, f_tar, funz_src,funz_tar)\n",
        "err = np.sum(np.square(v_tar - v_tar[p2p,:]))\n",
        "print(err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "divV5v4v3cl2"
      },
      "source": [
        "# landmarks\n",
        "n_land = 45\n",
        "n_evals = 20\n",
        "step = np.int(np.ceil(v_src.shape[0] / n_land))\n",
        "a = np.arange(0,v_src.shape[0],step)\n",
        "\n",
        "landmarks = np.zeros((v_src.shape[0], a.size))\n",
        "landmarks[a,np.arange(a.size)] = 1\n",
        "\n",
        "D = (np.matmul(evecs_src[:,0:n_evals].T, np.matmul(M_src.todense() , landmarks[:,0:n_land]))).T\n",
        "F = (np.matmul(evecs_tar[:,0:n_evals].T, np.matmul(M_tar.todense() , landmarks[:,0:n_land]))).T\n",
        "C =  np.matmul(np.linalg.pinv(D),F)\n",
        "\n",
        "plt.imshow(C)\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d9okpkc5Lzi"
      },
      "source": [
        "treesearch = sp.spatial.cKDTree(np.matmul(evecs_tar[:,0:n_evals],C))\n",
        "p2p = treesearch.query(evecs_src[:,0:n_evals], k=1)[1]\n",
        "funz_src = funz_tar[p2p]\n",
        "plot_pair(v_src, v_tar, f_src, f_tar, funz_src,funz_tar)\n",
        "err = np.sum(np.square(v_tar - v_tar[p2p,:]))\n",
        "print(err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ttg0B3wy-VCq"
      },
      "source": [
        "Here we implement Functional Maps as an optimization problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piGDB7xDAOnC"
      },
      "source": [
        "# Computing descriptors\n",
        "n_land =  20\n",
        "n_wks  =  2\n",
        "\n",
        "n_evals = 20\n",
        "\n",
        "# Landmarks\n",
        "step = np.int(np.ceil(v_src.shape[0] / n_land))\n",
        "a = np.arange(0,v_src.shape[0],step)\n",
        "landmarks = np.zeros((v_src.shape[0], a.size))\n",
        "landmarks[a,np.arange(a.size)] = 1\n",
        "\n",
        "# WKS\n",
        "d_src = WKS(v_src, f_src, evals_src, evecs_src, n_wks, 7)\n",
        "d_tar = WKS(v_tar, f_tar, evals_tar, evecs_tar, n_wks, 7)\n",
        "\n",
        "# Optimization Process\n",
        "desc_src = np.hstack((landmarks,d_src))\n",
        "desc_tar = np.hstack((landmarks,d_tar))\n",
        "\n",
        "# Descriptor normalization\n",
        "no = np.sqrt(np.diag(np.matmul(M_src.T.__matmul__(desc_src).T, desc_src)))\n",
        "no_s = np.tile(no.T,(v_src.shape[0],1))\n",
        "no_t = np.tile(no.T,(v_tar.shape[0],1))\n",
        "fct_src = np.divide(desc_src,no_s)\n",
        "fct_tar = np.divide(desc_tar,no_t)\n",
        "\n",
        "# Coefficents of the obtained descriptors\n",
        "Fct_src = np.matmul(M_src.T.__matmul__(evecs_src[:, 0:n_evals]).T, fct_src)\n",
        "Fct_tar = np.matmul(M_tar.T.__matmul__(evecs_tar[:, 0:n_evals]).T, fct_tar)\n",
        "\n",
        "# The relation between the two constant functions can be computed in a closed form\n",
        "constFct = np.zeros((n_evals,1))\n",
        "constFct[0, 0] = np.sign(evecs_src[0, 0] * evecs_tar[0, 0]) * np.sqrt(np.sum(M_tar)/np.sum(M_src))\n",
        "\n",
        "# Different way to compute Laplacian commutativity\n",
        "# Dlb = (np.tile(evals_src[0:n_evals], (n_evals, 1)) - np.tile(evals_tar[0:n_evals].T, (n_evals, 1)))**2\n",
        "# Dlb = np.float32(Dlb/tf.reduce_sum((Dlb**2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUVLx9_4NhhR"
      },
      "source": [
        "Play with different energy weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjSwxIYJ5e0y"
      },
      "source": [
        "# Energy weights\n",
        "a = 1e-1 # Descriptors preservation\n",
        "c = 1e-8 # Commutativity with Laplacian\n",
        "\n",
        "\n",
        "# Define tensorflow objects\n",
        "fs = tf.constant(Fct_src, dtype=tf.float32)\n",
        "ft = tf.constant(Fct_tar, dtype=tf.float32)\n",
        "evals = tf.constant(tf.linalg.tensor_diag(np.reshape(np.float32(evals_src[0:n_evals]), (n_evals,))), dtype=tf.float32)\n",
        "evalt = tf.constant(tf.linalg.tensor_diag(np.reshape(np.float32(evals_tar[0:n_evals]), (n_evals,))), dtype=tf.float32)\n",
        "\n",
        "# Initialize C\n",
        "C_ini = np.zeros((n_evals,n_evals))\n",
        "C_ini[0,0]=constFct[0,0]\n",
        "C = tf.Variable(tf.zeros((n_evals,n_evals), dtype=tf.float32))\n",
        "C.assign(C_ini)\n",
        "\n",
        "# Optimizer\n",
        "adam = tf.keras.optimizers.Adam(1e-1) # Optimization technique\n",
        "trainable_vars = [C]\n",
        "\n",
        "# Optimization\n",
        "for i in np.arange(0,1000):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss1 = a * tf.reduce_sum(((tf.matmul(C, fs) - ft) ** 2)) / 2 # Descriptor preservation\n",
        "        loss2 = c * tf.reduce_sum((tf.matmul(C, evals) - tf.matmul(evalt,C))**2) #tf.reduce_sum(((C ** 2) * Dlb) / 2)  # Commute with Laplacian\n",
        "        #loss3 = 1e-6 *  tf.reduce_sum(tf.square(tf.matmul(tf.transpose(C),C) - tf.eye(n_evals))) # Orthonormal C\n",
        "        loss = loss1  + loss2# + loss3\n",
        "\n",
        "    # Apply gradient\n",
        "    grad = tape.gradient(loss,trainable_vars)\n",
        "    tmp = adam.apply_gradients(zip(grad,trainable_vars))\n",
        "\n",
        "C = C.numpy()\n",
        "plt.imshow(C)\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "treesearch = sp.spatial.cKDTree(np.matmul(evecs_tar[:,0:n_evals],C))\n",
        "p2p = treesearch.query(evecs_src[:,0:n_evals], k=1)[1]\n",
        "funz_src = funz_tar[p2p]\n",
        "plot_pair(v_src, v_tar, f_src, f_tar, funz_src,funz_tar)\n",
        "err = np.sum(np.square(v_src - v_src[p2p,:]))\n",
        "print(err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhc2WSUQS8Xw"
      },
      "source": [
        "A common post-processing is ICP refinement. The main idea is that we can think to our spectral embedding as N-dimensional pointclouds. If my change of basis $C$ brings the two close enough, we can try to improve the alignment using the rigid registration algorithm called Iterative Closest Point (ICP). \n",
        "It is based on two steps. Given a pointcloud $M$ that I want to align to $N$:\n",
        "- Computing the Nearest Neighbor matching between $M$ and $N$\n",
        "- Computing the optimal roto-translation via SVD that align $M$ with $N$ using the given NN correspondence\n",
        "- Iterate until convergence\n",
        "\n",
        "It is prone to get stuck in local-minima, but if the pointclouds are alrady close enough, it can significatly improve the solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv-jXEQZ8BHC"
      },
      "source": [
        "print('ICP refine...')\n",
        "for k in np.arange(0,5):\n",
        "    treesearch = scipy.spatial.cKDTree(np.matmul(evecs_tar[:,0:n_evals],C))\n",
        "    matches = treesearch.query(evecs_src[:,0:n_evals], k=1)[1]\n",
        "    W = np.linalg.lstsq(evecs_src[matches, 0:n_evals],evecs_tar[:, 0:n_evals])[0]\n",
        "    d = np.linalg.svd(W)\n",
        "    C_ICP = np.matmul(np.matmul(d[0], np.eye(n_evals)), d[2])\n",
        "\n",
        "plt.imshow(C_ICP)\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "treesearch = sp.spatial.cKDTree(evecs_tar[:,0:n_evals])\n",
        "p2p = treesearch.query(np.matmul(evecs_src[:,0:n_evals], C_ICP), k=1)[1]\n",
        "funz_src = funz_tar[p2p]\n",
        "plot_pair(v_src, v_tar, f_src, f_tar, funz_src,funz_tar)\n",
        "err = np.sum(np.square(v_tar - v_tar[p2p,:]))\n",
        "print(err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcutF7d3UOdz"
      },
      "source": [
        "Finally, here we anticipate the ZoomOut refinement that you will see tomorrow. This let to compute a high dimensional $C$. It is based on two iterative steps.Given a $C$ of dimension $n \\times n$:\n",
        "- Convert it into a dense correspondence\n",
        "- Convert the dense correspondence into a $C$ of dimension $(n+1) \\times (n+1)$ \n",
        "\n",
        "These two simple steps produce an impressive improvement in the matching quality.\n",
        "\n",
        "Note: it can be applied to refine _any_ correspondence, also not directly obtained from Functional Maps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQkuKj10UKld"
      },
      "source": [
        "# More iterations => bigger map => higher frequencies => better quality\n",
        "n_iter = 10\n",
        "\n",
        "# ZOOMOUT\n",
        "C_iter = C_ICP\n",
        "for i in np.arange(0,n_iter):\n",
        "  # 1) Convert into dense correspondence\n",
        "  treesearch = sp.spatial.cKDTree(np.matmul(evecs_tar[:,0:n_evals+i], C_iter.T))\n",
        "  p2p = treesearch.query(evecs_src[:,0:n_evals+i], k=1)[1]\n",
        "\n",
        "  #2) Convert into C of dimension (n+1) x (n+1)\n",
        "  C_iter = np.matmul(np.linalg.pinv(evecs_src[:,0:n_evals+i+1]),evecs_tar[p2p,0:n_evals+i+1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYcTv7aeVMUy"
      },
      "source": [
        "plt.imshow(C_iter)\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "treesearch = sp.spatial.cKDTree(np.matmul(evecs_tar[:,0:n_evals+n_iter],C_iter.T))\n",
        "p2p = treesearch.query(evecs_src[:,0:n_evals+n_iter], k=1)[1]\n",
        "funz_src = funz_tar[p2p]\n",
        "plot_pair(v_src, v_tar, f_src, f_tar, funz_src,funz_tar)\n",
        "err = np.sum(np.square(v_tar - v_tar[p2p,:]))\n",
        "print(err)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
